import numpy as np
import pandas as pd
import ipa.src.preprocessing_utils
import ipa.src.correction_utils
import logging
import nd2
import glob
from datetime import datetime
import os
from ipa.src.snakemake_utils import create_logger_format,create_logger_workflows,predict_stardist
from trackpy.feature import filter_image
from trackpy.preprocessing import lowpass
from skimage.morphology import disk, white_tophat
import ipa.src.detection_utils as det
from tqdm import tqdm


# Define names of the input and output files
threads = int(config["threads"])

FILENAME = [x.split('/')[-1].replace(".nd2", "") for x in glob.glob(f"{config['folder_path']}/*.nd2")]
images = expand("{folder_path}/{filename}.nd2", folder_path=config['folder_path'], filename="{filename}")

formatted_image = config["save_path"]+"/formatted_image_{filename}.npy"

detection = "{path}/detections/detections_{filename}_cxy_{crop_sizexy}_cz_{crop_size_z}.csv"

detections_matched = expand("{path}/detections_matched/detections_matched_{filename}_cxy_{crop_sizexy}_cz_{crop_sizexy}.csv", filename=FILENAME,
         crop_sizexy=config["crop_size_xy"],
         path=config["save_path"])

detection_matched = "{path}/detections_matched/detections_matched_{filename}_cxy_{crop_sizexy}_cz_{crop_size_z}.csv"

h_param = "{path}/hmax_params/h_params_{filename}.npy"

n_s = "{path}/hmax_params/n_params_{filename}.npy"

metadata = config['save_path']+"/metadata/{filename}_metadata.npy"

labels = config["save_path"]+"/label_image_{filename}.npy"

detection_tracked = "{path}/tracks/tracks_{filename}.parquet"


max_threads = os.cpu_count() 

rule all:
    input:
        detections_matched

rule format_images:
    input:
        lambda wildcards: f"{config['folder_path']}{wildcards.filename}.nd2"
    output:
        formatted_image,
        metadata
    params:
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards),
        filename = '{filename}'
    run:
        im = nd2.imread(input[0])
        if len(im.shape) != 5:
            params.lo.info(f"Image {input[0].split('/')[-1]} has the wrong shape {im.shape}")
            logger.info(f"Image {input[0].split('/')[-1]} has the wrong shape {im.shape}")
            im = np.reshape(im,(im.shape[0]//15,15,2,976,976))

        met = nd2.ND2File(input[0])
        # im = np.load(input[0])
        logger.info(f"Loaded image {input[0]}")
        params.lo.info(f"Loaded image {input[0]}")
        logger.info(f"The image has a shape of {im.shape}")
        params.lo.info(f"The image has a shape of {im.shape}")
        footprint=disk(3)
        im = np.array(im, dtype=float)

        params.lo.info(f"The image was smoothed bandpass and white tophat was applied")
        for frame in tqdm(range(np.shape(im)[0])):
            for dim in range(np.shape(im)[2]):
                im[frame,:,dim,...] = lowpass(im[frame,:,dim,...])
                for plane in range(np.shape(im)[1]):
                        im[frame,plane,dim,...] = white_tophat(im[frame,plane,dim,...], footprint)
       
        logger.info(f"Reformatted the image to have a shape of {im.shape}")
        params.lo.info(f"Reformatted the image to have a shape of {im.shape}")
        np.save(output[0], im)
        np.save(output[1],list(met.voxel_size()))

rule compute_h_param:
    input:
        formatted_image
    output:
        h_param
    params:
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards)
    run:
        
        im_big = np.load(input[0])
        sample_size = 50
        # to fix
        h = np.zeros((np.shape(im_big)[2],sample_size))
        
        logger.info(f"Loaded image {input[0]} for h param computation")
        logger.info(f"The image has a shape of {im_big.shape}")

        params.lo.info(f"Loaded image {input[0]} for h param computation")
        params.lo.info(f"The image has a shape of {im_big.shape}")

        for l,frame in tqdm(enumerate(np.random.randint(0,im_big.shape[0]-1,size=sample_size))):
            for dim in range(np.shape(im_big)[2]):
                im = im_big[frame,:,dim,...]
                h[dim,l] = np.percentile(np.max(im,axis=0).flatten(),64)#ipa.src.preprocessing_utils.compute_h_param(np.expand_dims(np.max(im,axis=0),axis=0),frame=0,thresh=0.2)
        
        #average across the frames
        h = np.mean(h,axis=1)

        logger.info(f"Computed h param, the h param for channel 1 is {h[0]} and for channel 2 is {h[1]}")
        params.lo.info(f"Computed h param, the h param for channel 1 is {h[0]} and for channel 2 is {h[1]}")

        np.save(output[0],h)

rule compute_labels:
    input: images
    output: labels
    params: 
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards)
    priority:
        19
    threads:
	    max_threads
    run: 
        im_big = nd2.imread(input[0])
        if len(im_big.shape) == 5:
            im = im_big[0,:,1,...]
        else:
            im = im_big[:,1,...]
        #predict labels
        params.lo.info(f"Loaded image {input[0]} for labels computation")

        labels = predict_stardist(np.max(im,axis=0))

        np.save(output[0],labels)


rule compute_n_param:
    input: formatted_image,h_param,labels
    output: n_s
    params:
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards),
        crop_size_xy = config["crop_size_xy"],
        crop_size_z = config["crop_size_z"]
        
    run: 
        im_big = np.load(input[0])
        h_ = np.load(input[1])
        labels = np.load(input[2])
        n_labels = len(np.unique(labels[labels>0]))
        params.lo.info(f"Loaded image {input[0]} for n param computation")
        params.lo.info(f"There are {n_labels} labels in the image")

        n_final = np.zeros((np.shape(im_big)[2],1))

        if n_labels < 5 or n_labels > 50:
            params.lo.info(f"Number of labels is {n_labels} which is either too high ot too low, the paramter will be set to 50")
            n_labels = 50

        im_big = np.max(im_big,axis=0)
        for dim in range(np.shape(im_big)[1]):
            h = h_[dim]
            im = im_big[:,dim,...]
            for n in np.arange(1,10,0.5):
                n_detections= len(det.hmax_3D_dask(im,frame=0,channel=0,sd=h,n=n,crop_size_xy= params.crop_size_xy,crop_size_z= params.crop_size_z))
                if n_detections < n_labels:
                    n_final_1 = n
                    break
                elif n == 9.5:
                    n_final_1 = 9.5
            n_final[dim] = n_final_1
            params.lo.info(f"Computed n param, the n param for channel {dim} is {n_final_1} and the number of detections is {n_detections}")

        np.save(output[0],n_final)

rule compute_detections:
    input:
        formatted_image,
        h_param,
        n_s
    output:
        detection
    params:
        log_filename = lambda wildcards: f"{wildcards.filename}_cxy_{wildcards.crop_sizexy}_cz_{wildcards.crop_size_z}",
        log_path = config["save_path"],
        crop_size_xy = int(config["crop_size_xy"]),
        crop_size_z = int(config["crop_size_z"])
    threads: threads
    shell:
        """
        python -m ipa.src.run_detections_snakemake_dask_args --input_image {input[0]} --h_file {input[1]} --n_file {input[2]} --output_file {output} --log_filename {params.log_filename} --log_path {params.log_path} --crop_sizexy {params.crop_size_xy} --crop_sizez {params.crop_size_z} --threads {threads}

        """

# rule compute_tracks:
#     input: detection,metadata
#     output: detection_tracked
#     run: 
#         df = pd.read_parquet(input[0])
#         df['actual_frame'] = [i + 10*x[0] for x,i in zip(df.chunk_location.values,df.frame.values)]
#         pixel_size_xy = met[0]
#         pixel_size_z = met[2]
#         df['x'] = df['x_fitted_refined']*pixel_size_xy
#         df['y'] = df['y_fitted_refined']*pixel_size_xy
#         df['z'] = df['z_fitted_refined']*pixel_size_z

#         df = df[['actual_frame','x','y','z','channel']]
#         df.rename(columns={'actual_frame':'frame'},inplace=True)
#         df_tracks = tu.track(df,track_cost_cutoff=0.5,gap_closing_cost_cutoff=0.0,gap_closing_max_frame_count=0)
#         # Find the repeated track_id (matched points)
#         u, c = np.unique(df_tracks.track_id.values, return_counts=True)
#         dup = u[c > 1]
#         track_m = df_tracks[df_tracks.track_id.isin(dup)]
#         track_m.to_parquet(output[0],index=False)

rule compute_matching:
    input: detection,metadata
    output: detection_matched
    params:
        cutoff = config["cutoff"],
        lo = lambda wildcards: create_logger_workflows(wildcards)
    run: 
        logger.info(f"Matching detections for {input[0]}")
        params.lo.info(f"Matching detections for {input[0]}")
        df = pd.read_parquet(input[0])
        met = np.load(input[1])

        # add actual frame 

        df['actual_frame'] = [i + 10*x[0] for x,i in zip(df.chunk_location.values,df.frame.values)]

        pixel_size_xy = met[0]
        pixel_size_z = met[2]
        df['x_um'] = df['x_fitted_refined']*pixel_size_xy
        df['y_um'] = df['y_fitted_refined']*pixel_size_xy
        df['z_um'] = df['z_fitted_refined']*pixel_size_z

        logger.info(f'The pixel sized in xy is {pixel_size_xy} and in z is {pixel_size_z}')
        params.lo.info(f'The pixel sized in xy is {pixel_size_xy} and in z is {pixel_size_z}')

        m = []

        for frame in df.actual_frame.unique():
            df_sub = df[df.actual_frame == frame]
            df_sub.reset_index(inplace=True,drop=True)
            matched = ipa.src.correction_utils.assign_closest(df_sub[df_sub.channel == 0],df_sub[df_sub.channel == 1],cutoff=params.cutoff)
            df_channel_1 = df_sub[df_sub.channel == 0].copy()

            logger.info(f"Matched {len(matched)} detections")
            params.lo.info(f"Matched {len(matched)} detections")
            
            for i in matched:
                df_channel_1.loc[i[0],'dx'] = i[2]
                df_channel_1.loc[i[0],'dy'] = i[3]
                df_channel_1.loc[i[0],'dz'] = i[4]

            df_channel_1.dropna(inplace=True,axis=0)

            m.append(df_channel_1)

        df_channel_1 = pd.concat(m)
        df_channel_1.to_parquet(output[0],index=False)

# rule compute_tracks_matched:
#     input: detection_matched
#     output: detection_tracked_matched
#     run: 
#         df = pd.read_parquet(input[0])

#         df = df[['actual_frame','x_fitted_refined','y_fitted_refined','z_fitted_refined','channel']]
#         df.rename(columns={'actual_frame':'frame','x_fitted_refined':'x','y_fitted_refined':'y','z_fitted_refined':'z'},inplace=True)
#         df_tracks = tu.track(df,track_cost_cutoff=0.5,gap_closing_cost_cutoff=0.0,gap_closing_max_frame_count=0)
#         # Find the repeated track_id (matched points)
#         u, c = np.unique(df_tracks.track_id.values, return_counts=True)
#         dup = u[c > 1]
#         track_m = df_tracks[df_tracks.track_id.isin(dup)]
#         track_m.to_parquet(output[0],index=False)


EMAIL = config['email']

onsuccess:
   shell("mail -s 'DONE' {EMAIL} < {log}")

onerror:
   shell("mail -s 'ERROR' {EMAIL}")