import numpy as np
import pandas as pd
import ipa.src.preprocessing_utils
import ipa.src.correction_utils
import logging
import nd2
import glob
from datetime import datetime
import os
from ipa.src.snakemake_utils import create_logger_format,create_logger_workflow,predict_stardist
from trackpy.feature import filter_image
from trackpy.preprocessing import lowpass
from skimage.morphology import disk, white_tophat
import ipa.src.detection_utils as det



# Define names of the input and output files
threads = config["threads"]
FILENAME = [x.split('/')[-1].split('.')[0] for x in glob.glob(f"{config['folder_path']}/*.nd2")]

images = expand("{folder_path}/{filename}.nd2", folder_path=config['folder_path'], filename="{filename}")

formatted_image = config["save_path"]+"/formatted_image_{filename}_smoothing_{smoothing}.npy"

detection = "{path}/detections/detections_{filename}_method_{method}_cxy_{crop_sizexy}_cz_{crop_size_z}_smoothing_{smoothing}.csv"

detections_matched = expand("{path}/detections_matched/detections_matched_{filename}_method_{method}_cxy_{crop_sizexy}_cz_{crop_sizexy}_smoothing_{smoothing}.csv", filename=FILENAME,
         method=config["method"],
         crop_sizexy=config["crop_size_xy"],
         path=config["save_path"],
         smoothing = config["smoothing"])

detection_matched = "{path}/detections_matched/detections_matched_{filename}_method_{method}_cxy_{crop_sizexy}_cz_{crop_size_z}_smoothing_{smoothing}.csv"

h_param = "{path}/hmax_params/h_params_{filename}_smoothing_{smoothing}.npy"

n_s = "{path}/hmax_params/n_params_{filename}_smoothing_{smoothing}.npy"

metadata = config['save_path']+"/metadata/{filename}_smoothing_{smoothing}_metadata.npy"

smoothing = config['smoothing']

labels = config["save_path"]+"/label_image_{filename}_smoothing_{smoothing}.npy"

#create_l =

max_threads = os.cpu_count() 

rule all:
    input:
        detections_matched

rule format_images:
    input:
        lambda wildcards: f"{config['folder_path']}{wildcards.filename}.nd2"
    output:
        temp(formatted_image),
        metadata
    params:
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards),
        smoothing = '{smoothing}',
        filename = '{filename}'
    run:
        im = nd2.imread(input[0])
        if im.shape != (4, 15, 2, 976, 976):
            im = im.reshape(4,im.shape[0]//4,im.shape[1],im.shape[2],im.shape[3])
        else:
            im = im
        met = nd2.ND2File(input[0])

        logger.info(f"Loaded image {input[0]}")
        params.lo.info(f"Loaded image {input[0]}")
        logger.info(f"The image has a shape of {im.shape}")
        params.lo.info(f"The image has a shape of {im.shape}")
        footprint=disk(2)

        im = np.array(im, dtype=float)

        if int(wildcards.smoothing) == 0:
            im = im
            params.lo.info(f"The image was not smoothed")
        elif int(wildcards.smoothing) == 1:
            params.lo.info(f"The image was smoothed (bandpass+boxcar)")
            for frame in range(np.shape(im)[0]):
                for dim in range(np.shape(im)[2]):
                    im[frame,:,dim,...] = filter_image(im[frame,:,dim,...],diameter = int(config['crop_size_xy'][0]))
                
        elif int(wildcards.smoothing) == 2:
            params.lo.info(f"The image was smoothed (bandpass + boxcar) and white tophat was applied")
            for frame in range(np.shape(im)[0]):
                for dim in range(np.shape(im)[2]):
                    im[frame,:,dim,...] = filter_image(im[frame,:,dim,...],diameter = int(config['crop_size_xy'][0]))
                    for plane in range(np.shape(im)[1]):
                            im[frame,plane,dim,...] = white_tophat(im[frame,plane,dim,...], footprint)

        elif int(wildcards.smoothing) == 3:
            params.lo.info(f"The image was smoothed bandpass and white tophat was applied")
            for frame in range(np.shape(im)[0]):
                for dim in range(np.shape(im)[2]):
                    im[frame,:,dim,...] = lowpass(im[frame,:,dim,...])
                    for plane in range(np.shape(im)[1]):
                            im[frame,plane,dim,...] = white_tophat(im[frame,plane,dim,...], footprint)
        elif int(wildcards.smoothing) == 4:
            params.lo.info(f"The image had a white tophat applied and smoothed with a bandpass")
            for frame in range(np.shape(im)[0]):
                for dim in range(np.shape(im)[2]):
                    for plane in range(np.shape(im)[1]):
                            im[frame,plane,dim,...] = white_tophat(im[frame,plane,dim,...], footprint)
                    im[frame,:,dim,...] = filter_image(im[frame,:,dim,...],diameter = 11)
        else:
            raise ValueError("Smoothing parameter should be 0, 1, 2, 3 or 4")

        logger.info(f"Reformatted the image to have a shape of {im.shape}")
        params.lo.info(f"Reformatted the image to have a shape of {im.shape}")
        np.save(output[0], im)
        np.save(output[1],list(met.voxel_size()))


rule compute_h_param:
    input:
        formatted_image
    output:
        h_param
    params:
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards)
    run:
        
        im_big = np.load(input[0])
        h = np.zeros((np.shape(im_big)[2],np.shape(im_big)[0]))
        
        logger.info(f"Loaded image {input[0]} for h param computation")
        logger.info(f"The image has a shape of {im_big.shape}")

        params.lo.info(f"Loaded image {input[0]} for h param computation")
        params.lo.info(f"The image has a shape of {im_big.shape}")
        for frame in range(np.shape(im_big)[0]):
            for dim in range(np.shape(im_big)[2]):
                im = im_big[frame,:,dim,...]
                h[dim,frame] = ipa.src.preprocessing_utils.compute_h_param(np.expand_dims(np.max(im,axis=0),axis=0),frame=0,thresh=0.2)
        
        logger.info(f"Computed h param, the h param for channel 1 is {h[0]} and for channel 2 is {h[1]}")
        params.lo.info(f"Computed h param, the h param for channel 1 is {h[0]} and for channel 2 is {h[1]}")

        np.save(output[0],h)

rule compute_labels:
    input: images
    output: temp(labels)
    params: 
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards)
    priority:
        19
    threads:
	    max_threads
    run: 
        im_big = nd2.imread(input[0])
        if len(im_big.shape) == 5:
            im = im_big[0,:,1,...]
        else:
            im = im_big[:,1,...]
        #predict labels
        params.lo.info(f"Loaded image {input[0]} for labels computation")

        labels = predict_stardist(np.max(im,axis=0))

        np.save(output[0],labels)


rule compute_n_param:
    input: formatted_image,h_param,labels
    output: n_s
    params:
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards),
        thresh = config["thresh"]
    run: 
        im_big = np.load(input[0])
        h_ = np.load(input[1])
        labels = np.load(input[2])
        n_labels = len(np.unique(labels[labels>0]))
        params.lo.info(f"Loaded image {input[0]} for n param computation")
        params.lo.info(f"There are {n_labels} labels in the image")

        n_final = np.zeros((np.shape(im_big)[2],np.shape(im_big)[0]))

        if n_labels < 5 or n_labels > 50:
            params.lo.info(f"Number of labels is {n_labels} which is either too high ot too low, the paramter will be set to 50")
            n_labels = 50

        for frame in range(np.shape(im_big)[0]):
            for dim in range(np.shape(im_big)[2]):
                h = h_[dim,frame]
                im = im_big[frame,:,dim,...]
                for n in np.arange(1,10,0.5):
                    n_detections= len(det.hmax_3D(raw_im= np.expand_dims(im,axis=0),frame=0,sd=h,n = n,thresh = params.thresh,threads = 1,fitting=False))
                    if n_detections < n_labels:
                        n_final_1 = n
                        break
                    elif n == 9.5:
                        n_final_1 = 9.5
                n_final[dim,frame] = n_final_1
                params.lo.info(f"Computed n param, the n param for channel {dim} is {n_final_1} and the number of detections is {n_detections}")

        np.save(output[0],n_final)

rule compute_detections:
    input:
        formatted_image,
        h_param,
        n_s
    output:
        detection
    params:
        thresh = config["thresh"],
        log_filename = lambda wildcards: f"{wildcards.filename}_method_{wildcards.method}_cxy_{wildcards.crop_sizexy}_cz_{wildcards.crop_size_z}",
        log_path = config["save_path"],
        method = config["method"]
    threads: threads
    script:
        "ipa/src/run_detections_snakemake.py"

rule compute_matching:
    input: detection,metadata
    output: detection_matched
    params:
        cutoff = config["cutoff"],
        lo = lambda wildcards: create_logger_workflow(wildcards)
    run: 
        logger.info(f"Matching detections for {input[0]}")
        params.lo.info(f"Matching detections for {input[0]}")
        df = pd.read_csv(input[0])
        met = np.load(input[1])

        pixel_size_xy = met[0]
        pixel_size_z = met[2]
        df['x_um'] = df['x_fitted_refined']*pixel_size_xy
        df['y_um'] = df['y_fitted_refined']*pixel_size_xy
        df['z_um'] = df['z_fitted_refined']*pixel_size_z

        logger.info(f'The pixel sized in xy is {pixel_size_xy} and in z is {pixel_size_z}')
        params.lo.info(f'The pixel sized in xy is {pixel_size_xy} and in z is {pixel_size_z}')
        
        m = []

        for frame in df.frame.unique():
            df_sub = df[df.frame == frame]
            df_sub.reset_index(inplace=True,drop=True)
            matched = ipa.src.correction_utils.assign_closest(df_sub[df_sub.channel == 0],df_sub[df_sub.channel == 1],cutoff=params.cutoff)
            df_channel_1 = df_sub[df_sub.channel == 0].copy()

            logger.info(f"Matched {len(matched)} detections")
            params.lo.info(f"Matched {len(matched)} detections")
            
            for i in matched:
                df_channel_1.loc[i[0],'dx'] = i[2]
                df_channel_1.loc[i[0],'dy'] = i[3]
                df_channel_1.loc[i[0],'dz'] = i[4]

            df_channel_1.dropna(inplace=True,axis=0)

            m.append(df_channel_1)

        df_channel_1 = pd.concat(m)
        df_channel_1.to_csv(output[0],index=False)

EMAIL = "nessim.louafi@fmi.ch"

onsuccess:
   shell("mail -s 'DONE' {EMAIL} < {log}")

onerror:
   shell("mail -s 'ERROR' {EMAIL}")