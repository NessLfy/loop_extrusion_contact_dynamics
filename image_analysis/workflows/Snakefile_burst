import numpy as np
import pandas as pd
import logging
import nd2
import glob
from datetime import datetime
import os
from snakemake_utils import create_logger_format,create_logger_workflows,predict_stardist_complete_burst
from tqdm import tqdm
from skimage.measure import regionprops_table
import dask.array as da
from zarr_tools import convert
import matplotlib.pyplot as plt
import dask 
from skimage.morphology import disk,white_tophat
from pathlib import Path
import re
dask.config.set(scheduler='threads', num_workers=1)

# Define names of the input and output files
threads = int(config["threads"])

FILENAME = [x.split('/')[-1].replace(".nd2", "") for x in glob.glob(f"{config['folder_path']}/*.nd2")]

plots = expand("{path}/plots/{filename}/{filename}_cxy_{crop_sizexy}_cz_{crop_size_z}_method_{method}_fit_{raw}_image_sanity_check.png", 
        filename=FILENAME,
        path=config["save_path"], 
        crop_sizexy=config["crop_size_xy"], 
        crop_size_z=config["crop_size_z"],
        raw = config["raw"],
        method = config["method"])

plot = "{path}/plots/{filename}/{filename}_cxy_{crop_sizexy}_cz_{crop_size_z}_method_{method}_fit_{raw}_image_sanity_check.png"

detection = "{path}/detections/detections_{filename}_cxy_{crop_sizexy}_cz_{crop_size_z}_method_{method}_fit_{raw}_image.parquet"

detections = expand("{path}/detections/detections_{filename}_cxy_{crop_sizexy}_cz_{crop_size_z}_method_{method}_fit_{raw}_image.parquet", filename=FILENAME,
         crop_sizexy=config["crop_size_xy"],
         crop_size_z=config["crop_size_z"],
         path=config["save_path"],
         raw=config["raw"],
         method=config["method"])


labels = config["save_path"]+"/labels/label_image_{filename}.zarr"

labels_tracked = expand(config['save_path']+"/label_image_tracked/label_image_tracked_{filename}.parquet", filename=FILENAME)

label_tracked = config['save_path']+"/label_image_tracked/label_image_tracked_{filename}.parquet"

labels_full = ancient(directory(config['save_path']+"/labels/label_image_full_{filename}.zarr"))

max_threads = os.cpu_count() 

rule all:
    input:
        detections,
        labels_tracked,
        plots

rule compute_detections:
    input:
        lambda wildcards: f"{config['folder_path']}{wildcards.filename}.nd2",
        labels_full
    output:
        detection
    params:
        log_filename = lambda wildcards: f"{wildcards.filename}_cxy_{wildcards.crop_sizexy}_cz_{wildcards.crop_size_z}",
        log_path = config["save_path"],
        crop_size_xy = int(config["crop_size_xy"]),
        crop_size_z = int(config["crop_size_z"]),
        method = lambda wildcards: wildcards.method,
        raw = lambda wildcards: wildcards.raw
    threads: threads
    shell:
        """
        python -m ipa.scripts.spot_detection_no_channel --input_image {input[0]} --output_file {output} --labels {input[1]} --crop_size_xy {params.crop_size_xy} --crop_size_z {params.crop_size_z} --threads {threads} --method {params.method} --raw {params.raw}
        """

rule compute_labels_full:
    input: lambda wildcards: f"{config['folder_path']}{wildcards.filename}.nd2"
    output: labels_full
    params: 
        lo = lambda wildcards: create_logger_format(config['save_path'], wildcards)
    priority:
        19
    threads:
	    max_threads
    run: 
        im = nd2.imread(input[0],dask=True)
        print(f"Loaded image {input[0]} with shape {im.shape}")
        #predict labels
        params.lo.info(f"Loaded image {input[0]} for labels computation")

        labels = predict_stardist_complete_burst(im)

        labels = da.from_array(labels, chunks=(1,labels.shape[1],labels.shape[2]))

        convert.to_zarr(
            labels,
            channel_axis=0,
            path=output[0], 
            steps=4
        )

rule track_cells:
    input:
        labels_full
    output:
        label_tracked
    shell:
        """
        python -m ipa.scripts.track_cells --labels_file {input} --output_file {output}
        """

rule sanity_check:
    input:
        lambda wildcards: f"{config['folder_path']}{wildcards.filename}.nd2",detection
    output:
        plot
    run:
        dask.config.set(scheduler='threads', num_workers=1)
        im = nd2.imread(input[0],dask=True)
        
        df = pd.read_parquet(input[1])
        for i in np.random.randint(0,im.shape[0],5):
            fig,ax = plt.subplots(1,1,figsize=(8,5))
            im_max_1 = np.max(im[i,:,...],axis=0).compute()


            im_max_1 = white_tophat(im_max_1,disk(2))


            vmin1 = np.quantile(im_max_1,0.10)
            vmax1 = np.quantile(im_max_1,0.999)

            ax.imshow(im_max_1,cmap='gray',vmin=vmin1,vmax=vmax1)
            df_sub_c1 = df[(df.frame==i)]

            df_sub_c1 = df_sub_c1.groupby('label').apply(lambda x: x.loc[x['snr_original'].idxmax()])

            ax.scatter(df_sub_c1.x,df_sub_c1.y,edgecolors='r',facecolors='none',s=100)


            ax.axis('off')

            fig.suptitle(f'Frame {i}')
            plt.tight_layout()
            plt.savefig(output[0].strip('.png')+'_frame_'+str(i)+'.png',dpi=300)
            plt.close()
        plt.savefig(output[0],dpi=300)


EMAIL = config['email']

onsuccess:
   shell("mail -s 'DONE' {EMAIL} < {log}")

onerror:
   shell("mail -s 'ERROR' {EMAIL}")
